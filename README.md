This is a Transformer model that generates human-like musical dynamics given a MIDI file. In other words, it performs music like a human!

<img width="468" alt="Model Architecture" src="https://github.com/user-attachments/assets/37003f9a-9442-4682-aec9-83c7abddcc62">

See [StyleNet](https://github.com/imalikshake/StyleNet) or [MIDI-DDSP](https://github.com/magenta/midi-ddsp) for similar projects.

If you wish to learn more, here is the corresponding paper: Auto-Dynamics.docx

# Dataset
The dataset used is [MAESTRO V3.0.0](https://magenta.tensorflow.org/datasets/maestro) under a [Creative Commons license](https://creativecommons.org/licenses/by-nc-sa/4.0/).

# Examples
Here are some sample audios with a human performance, a model-generated performance, and a dynamics-removed version side-by-side:

